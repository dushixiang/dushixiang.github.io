<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on 没有理想的人不伤心</title>
    <link>https://typesafe.cn/posts/</link>
    <description>Recent content in Posts on 没有理想的人不伤心</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Sat, 16 Oct 2021 22:55:00 +0800</lastBuildDate><atom:link href="https://typesafe.cn/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Java 反序列化漏洞（二）新版本JDK利用方式和Shiro举例</title>
      <link>https://typesafe.cn/posts/java-serialization-vulnerability-2/</link>
      <pubDate>Sat, 16 Oct 2021 22:55:00 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/java-serialization-vulnerability-2/</guid>
      <description>声明    本文章中所有内容仅供学习交流，严禁用于非法用途，否则由此产生的一切后果均与作者无关。
新的希望    0x00    在上一节中我们介绍了 Java 反序列化漏洞的成因和利用 commons-collections 3.1 搭配 sun.reflect.annotation.AnnotationInvocationHandler 实现远程命令执行的方式。但sun.reflect.annotation.AnnotationInvocationHandler 的问题已经在最新版 jdk 中修复，可利用范围仅能够局限于旧版本的jdk。经过安全人员的审计，另一个类 javax.management.BadAttributeValueExpException 出现在了安全人员的视野。
javax.management.BadAttributeValueExpException 继承自 java.lang.Exception，java.lang.Exception 继承自 java.lang.Throwable，而 java.lang.Throwable 实现了 java.io.Serializable。因此 javax.management.BadAttributeValueExpException 符合了 可序列化 这个要求，同样的它也增加了 readObject 方法，这个类的完整代码如下：
package javax.management; import java.io.IOException; import java.io.ObjectInputStream; /** * Thrown when an invalid MBean attribute is passed to a query * constructing method. This exception is used internally by JMX * during the evaluation of a query.</description>
    </item>
    
    <item>
      <title>Java 反序列化漏洞（一）Serializable</title>
      <link>https://typesafe.cn/posts/java-serialization-vulnerability-1/</link>
      <pubDate>Thu, 14 Oct 2021 23:00:00 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/java-serialization-vulnerability-1/</guid>
      <description>声明    本文章中所有内容仅供学习交流，严禁用于非法用途，否则由此产生的一切后果均与作者无关。
序列化的定义    序列化是指将数据结构或对象状态转换成可取用格式，以留待后续在相同或另一台计算机环境中，能恢复原先状态的过程。依照序列化格式重新获取字节的结果时，可以利用它来产生与原始对象相同语义的副本。
Java 中的序列化    Java 自身提供了序列化的功能，需要实现 java.io.Serializable 接口，标明该对象是可序列化的。 java.io.Serializable 是一个空接口，不需要对象实现方法。
以下面这段代码为例，展示了一个对象的序列化和反序列化的过程。
import java.io.*; import java.nio.charset.StandardCharsets; import java.util.Base64; public class Eval0 { public static class Command implements Serializable { private String cmd; public String getCmd() { return cmd; } public void setCmd(String cmd) { this.cmd = cmd; } } public static void main(String[] args) throws Exception { // 定义一个对象  Command command = new Command(); command.</description>
    </item>
    
    <item>
      <title>分享一款非常好用的kafka可视化web管理工具</title>
      <link>https://typesafe.cn/posts/share-a-kafka-web-manager/</link>
      <pubDate>Sun, 13 Jun 2021 00:00:00 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/share-a-kafka-web-manager/</guid>
      <description>使用过kafka的小伙伴应该都知道kafka本身是没有管理界面的，所有操作都需要手动执行命令来完成。但有些命令又多又长，如果没有做笔记，别说是新手，就连老手也不一定能记得住，每次想要使用的时候都要上网搜索一下。有些崇尚geek精神的人或许觉得命令行才是真爱，但使用一款好用的可视化管理工具真的可以极大的提升效率。
今天给大家介绍的这款工具叫做kafka-map，是我针对日常工作中高频使用的场景开发的，使用了这款工具之后就不必费心费力的去查资料某个命令要怎么写，就像是：“给编程插上翅膀，给kafka装上导航”。
kafka-map 介绍    kafka map是使用Java11和React开发的一款kafka可视化工具。
目前支持的功能有：
 多集群管理 集群状态监控（分区数量、副本数量、存储大小、offset） 主题创建、删除、扩容（删除需配置delete.topic.enable = true） broker状态监控 消费者组查看、删除 重置offset 消息查询（支持String和json方式展示） 发送消息（支持向指定的topic和partition发送字符串消息）  功能截图    添加集群    集群管理    broker    主题管理    消费组    查看消费组已订阅主题    topic详情——分区    topic详情——broker    topic详情——消费组    topic详情——消费组重置offset    topic详情——配置信息    生产消息    消费消息    docker 方式安装    一行命令即可完成安装</description>
    </item>
    
    <item>
      <title>openstack victoria版安装</title>
      <link>https://typesafe.cn/posts/install-openstack-victoria/</link>
      <pubDate>Sat, 12 Jun 2021 00:00:00 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/install-openstack-victoria/</guid>
      <description>近期公司业务需求，需要安装一套Openstack环境学习，看了一下现在已经出了wallaby版了，我果断选择了上一个版本victoria。因为没有足够多的物理服务器了，只好找了一台64核256G内存6T硬盘的机器来创建几台虚拟机来搭环境了。
实验环境    此次实验使用到了三台虚拟机，都是使用centos8系统，一台机器当作控制和网络节点，另外两台当作计算节点，使用OVS+VLAN的网络模式，eth0作为管理网络，eth1互相连接到OVS网桥上模拟trunk网卡，controller多增加一个eth2用于访问外部网络。
   节点 作用 eth0 eth1 eth2     controller 控制节点、网络节点 172.16.10.100 无IP 桥接，无IP   compute-101 计算节点 172.16.10.101 无IP ❌   compute-102 计算节点 172.16.10.102 无IP ❌    安装虚拟机    安装依赖    安装KVM和Linux网桥
yum install -y qemu-kvm libvirt virt-install bridge-utils virt-manager dejavu-lgc-sans-fonts  dejavu-lgc-sans-fonts用于解决 virt-manaer 乱码
 启动
systemctl enable libvirtd &amp;amp;&amp;amp; systemctl start libvirtd 安装OVS</description>
    </item>
    
    <item>
      <title>KVM 虚拟机磁盘扩容</title>
      <link>https://typesafe.cn/posts/kvm-disk-resize/</link>
      <pubDate>Mon, 31 May 2021 19:20:00 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/kvm-disk-resize/</guid>
      <description>一、镜像扩容    注意：需要先关闭虚拟机才能操作，+号前面有空格，后面没有空格。
qemu-img resize test.qcow2 +80G 原镜像磁盘大小20GB，扩容完成后可使用以下命令查看
qemu-img info test.qcow2 输出
image: test.qcow2 file format: qcow2 virtual size: 100G (107374182400 bytes) disk size: 885M cluster_size: 65536 Format specific information: compat: 1.1 lazy refcounts: false refcount bits: 16 corrupt: false 二、Windows磁盘扩容    Windows磁盘扩容比较方便，进入 计算机管理&amp;gt;磁盘管理 找到新增的分区把它添加到需要的分区即可。
三、Linux磁盘扩容    启动虚拟机后，进入虚拟机控制台，使用fdisk -l命令查看磁盘信息。
Disk /dev/vda: 100 GiB, 107374182400 bytes, 209715200 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: dos Disk identifier: 0xe11f7f01 Device Boot Start End Sectors Size Id Type /dev/vda1 * 2048 2099199 2097152 1G 83 Linux /dev/vda2 2099200 41943039 39843840 19G 8e Linux LVM Disk /dev/mapper/cl-root: 17 GiB, 18249416704 bytes, 35643392 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disk /dev/mapper/cl-swap: 2 GiB, 2147483648 bytes, 4194304 sectors Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes 可以看到这台虚拟机的磁盘大小已经有100GB了，但分区大小还是没有变化，只有初始大小20GB。</description>
    </item>
    
    <item>
      <title>Linux虚拟化技术KVM</title>
      <link>https://typesafe.cn/posts/linux-kvm/</link>
      <pubDate>Sat, 29 May 2021 17:07:00 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/linux-kvm/</guid>
      <description>在Windows平台上我们习惯于使用VmWare或者virtual box来实现虚拟化，虽然它们拥有Linux版本，但大多数企业都选择了使用KVM来做Linux平台的虚拟化，因此学习掌握KVM是一项必不可少的技能。
安装KVM    以centos为例，下面是安装KVM虚拟化的命令。
yum install -y qemu-kvm libvirt virt-install bridge-utils 这么多软件都是什么作用？
   软件 作用     qemu-kvm 整合了QEMU 和 KVM 的一个软件。   libvirt 封装了QEMU的接口，可以更加方便的操作虚拟机，并且提供了很多种编程语言的SDK。   virt-install 用来创建虚拟机的命令行工具。   bridge-utils Linux网桥，用来配置虚拟机的桥接网络。    kvm、qemu、qemu-kvm和libvirt到底有什么关系？
KVM（Kernel Virtual Machine）是Linux的一个内核驱动模块，它需要CPU的支持，采用硬件辅助虚拟化技术Intel-VT、AMD-V；内存相关如Intel的EPT和AMD的RVI技术，使得它能够让Linux主机成为一个Hypervisor（虚拟机监控器）。
QEMU是一个纯软件实现的虚拟机，它可以模拟CPU、内存、磁盘等其他硬件，让虚拟机认为自己底层就是硬件，其实这些都是QEMU模拟的，虚拟机的所有操作都要经过QEMU转译一层，也就导致了QEMU本身的性能较差。
qemu-kvm是QEMU整合了KVM，把CPU虚拟化和内存虚拟化交给了KVM来做，自己来模拟IO设备，例如网卡和磁盘。这一套组合拳打下来，性能损失大大降低，相较于直接使用硬件，带来的损耗大概在1%-2%之间。
libvirt是目前使用最为广泛的对KVM虚拟机进行管理的工具和API。Libvirtd是一个daemon进程，可以被本地的virsh调用，也可以被远程的virsh调用，Libvirtd调用qemu-kvm操作虚拟机。
启动libvirt
systemctl start libvirtd systemctl enable libvirtd 如果你不想使用命令行工具来管理虚拟机，可以安装 virt-manager 。
yum install -y virt-manager 在支持x11转发的ssh客户端（例如：MobaXterm）上可以直接输入 virt-manager 来启动。
虚拟网络类型    和vmware类型，kvm也支持多种类型的网络，主要分为三种。</description>
    </item>
    
    <item>
      <title>容器网络——如何为docker添加网卡？</title>
      <link>https://typesafe.cn/posts/how-to-add-port-for-docker/</link>
      <pubDate>Sun, 23 May 2021 13:37:00 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/how-to-add-port-for-docker/</guid>
      <description>之前我们介绍Network Namespace（以下简称netns）和veth pair时说过docker是使用这些技术来实现的网络隔离，今天我们就来一探究竟，看下docker到底是如何做到的。
启动一个无网络的容器    首先我们使用 --net=none 参数启动一个无网络的容器，为了方便调试，这里我们使用了centos镜像。
docker run -itd --name centos-test --net=none centos 启动成功之后我们进入容器内部确认一下是否无网卡。
[root@localhost ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 28dc2e8853df centos &amp;#34;/bin/bash&amp;#34; 24 seconds ago Up 23 seconds centos-test [root@localhost ~]# docker exec -it 28dc2e8853df bash [root@28dc2e8853df /]# ip a 1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever 可以看到确实只有一个本地环回网卡。</description>
    </item>
    
    <item>
      <title>使用libvirt-java采集KVM虚拟机状态信息</title>
      <link>https://typesafe.cn/posts/collect-vm-stats-by-libvirt-java/</link>
      <pubDate>Wed, 19 May 2021 20:18:20 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/collect-vm-stats-by-libvirt-java/</guid>
      <description>虚拟化开发相较于普通开发是一个冷门的方向，大多数是使用Python开发，其中使用Java来做虚拟化的少之又少，资料更是少的可怜，为了实现需求我也是踩了不少坑，今天就为大家分享一下如何使用 libvirt-java 来采集KVM虚拟机的资源使用信息。
CPU使用率    libvirt并没有直接提供获取虚拟机CPU使用率的接口，需要我们自己来计算，网上分享的代码或者公式五花八门，大部分都是错误的，经过我的测试，找到了一个相对准确的计算公式。
cpu_usage = (cpu_time_now - cpu_time_t_second_ago) * 100 / (t * vCpus * 10^9) Java代码如下
// t秒前的CPU时间 long c1 = domain.getInfo().cpuTime; Thread.sleep(1000); // 当前CPU时间 long c2 = domain.getInfo().cpuTime; // 虚拟CPU数量 int vCpus = domain.getMaxVcpus(); // t 为1秒 Double cpuUsage = 100 * (c2 - c1) / (1 * vCpus * Math.pow(10, 9)); log.debug(&amp;#34;虚拟机[{}]CPU使用率为: {}&amp;#34;, uuid, cpuUsage); 内存使用率    不要使用domain.getInfo()返回的 memory字段，虽然它注释写的是the memory in KBytes used by the domain，但它的意思真的不是虚拟机内部进程已使用的内存大小，而是从宿主机器的角度来看分配给这个虚拟机的内存它使用了多少，如果没有特殊配置，它会和maxMem字段的值是相同的。</description>
    </item>
    
    <item>
      <title>基于kafka实现延迟队列</title>
      <link>https://typesafe.cn/posts/kafka-delay-queue/</link>
      <pubDate>Sun, 18 Apr 2021 00:08:38 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/kafka-delay-queue/</guid>
      <description>基于kafka实现延迟队列    kafka作为一个使用广泛的消息队列，很多人都不会陌生，但当你在网上搜索“kafka 延迟队列”，出现的都是一些讲解时间轮或者只是提供了一些思路，并没有一份真实可用的代码实现，今天我们就来打破这个现象，提供一份可运行的代码，抛砖引玉，吸引更多的大神来分享。
基于kafka如何实现延迟队列？    想要解决一个问题，我们需要先分解问题。kafka作为一个高性能的消息队列，只要消费能力足够，发出的消息都是会立刻收到的，因此我们需要想一个办法，让消息延迟发送出去。
网上已经有大神给出了如下方案：
 在发送延迟消息时不直接发送到目标topic，而是发送到一个用于处理延迟消息的topic，例如delay-minutes-1 写一段代码拉取delay-minutes-1中的消息，将满足条件的消息发送到真正的目标主题里。  就像画一匹马一样简单。
方案是好的，但是我们还需要更多细节。
完善细节    问题出在哪里？
问题出在延迟消息发出去之后，代码程序就会立刻收到延迟消息，要如何处理才能让延迟消息等待一段时间才发送到真正的topic里面。
可能有同学会觉得很简单嘛，在代码程序收到消息之后判断条件不满足，就调用sleep方法，过了一段时间我再进行下一个循环拉取消息。
真的可行吗?
一切好像都很美好，但这是不可行的。
这是因为在轮询kafka拉取消息的时候，它会返回由max.poll.records配置指定的一批消息，但是当程序代码不能在max.poll.interval.ms配置的期望时间内处理这些消息的话，kafka就会认为这个消费者已经挂了，会进行rebalance，同时你这个消费者就无法再拉取到任何消息了。
举个例子：当你需要一个24小时的延迟消息队列，在代码里面写下了Thread.sleep(1000*60*60*24);，为了不发生rebalance，你把max.poll.interval.ms 也改成了1000*60*60*24，这个时候你或许会感觉到一丝丝的怪异，我是谁？我在哪？我为什么要写出来这样的代码？
其实我们可以更优雅的处理这个问题。
KafkaConsumer 提供了暂停和恢复的API函数，调用消费者的暂停方法后就无法再拉取到新的消息，同时长时间不消费kafka也不会认为这个消费者已经挂掉了。另外为了能够更加优雅，我们会启动一个定时器来替换sleep。，完整流程如下图，当消费者发现消息不满足条件时，我们就暂停消费者，并把偏移量seek到上一次消费的位置以便等待下一个周期再次消费这条消息。
Java代码实现    import com.fasterxml.jackson.core.JsonProcessingException; import com.fasterxml.jackson.databind.JsonNode; import com.fasterxml.jackson.databind.ObjectMapper; import org.apache.kafka.clients.consumer.*; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.common.TopicPartition; import org.apache.kafka.common.serialization.StringDeserializer; import org.apache.kafka.common.serialization.StringSerializer; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import org.springframework.boot.test.context.SpringBootTest; import java.time.Duration; import java.util.*; import java.util.concurrent.ExecutionException; @SpringBootTest public class DelayQueueTest { private KafkaConsumer&amp;lt;String, String&amp;gt; consumer; private KafkaProducer&amp;lt;String, String&amp;gt; producer; private volatile Boolean exit = false; private final Object lock = new Object(); private final String servers = &amp;#34;&amp;#34;; @BeforeEach void initConsumer() { Properties props = new Properties(); props.</description>
    </item>
    
    <item>
      <title>Java的奇技淫巧</title>
      <link>https://typesafe.cn/posts/java-unexpected-features/</link>
      <pubDate>Sat, 13 Mar 2021 13:49:38 +0800</pubDate>
      
      <guid>https://typesafe.cn/posts/java-unexpected-features/</guid>
      <description>Java是一种广泛使用的计算机编程语言、面向对象、泛型编程的特性，广泛应用于企业级Web应用开发和移动应用开发。
1995年3月23日Sun公司发布了Java，至今已有近26年，可以说是一门十分成熟的开发语言了，但在某些不为人知的地方存在着一些意料之外的特性。
Java的保留关键字 goto和const    在Java里面没有goto这个功能，但它作为保留字是无法当做变量来使用的，const也是同样。
int goto = 0; int const = 0; 上面这两行代码的写法存在问题，无法正常编译通过。
Java标签Label    上面说了在Java里面没有goto这个功能，但为了处理多重循环引入了Label，目的是为了在多重循环中方便的使用 break 和coutinue ，但好像在其他地方也可以用。
outerLoop: while (true) { System.out.println(&amp;#34;I&amp;#39;m the outer loop&amp;#34;); int i = 0; while (true) { System.out.println(&amp;#34;I am the inner loop&amp;#34;); i++; if (i &amp;gt;= 3) { break outerLoop; } } } System.out.println(&amp;#34;Complete the loop&amp;#34;); // 输出 I&amp;#39;m the outer loop I am the inner loop I am the inner loop I am the inner loop Complete the loop test: { System.</description>
    </item>
    
    <item>
      <title>Linux 环回网络接口</title>
      <link>https://typesafe.cn/posts/linux-loopback/</link>
      <pubDate>Thu, 28 Jan 2021 22:59:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/linux-loopback/</guid>
      <description>在开发或者调试时，我们经常需要和本地的服务器进行通信，例如启动nginx之后，在浏览器输入lcoalhost或者127.0.0.1就可以访问到本机上面的http服务。
Linux是如何访问本机IP的？    大多数操作系统都在网络层实现了环回能力，通常是使用一个虚拟的环回网络接口来实现。这个虚拟的环回网络接口看着像是一个真实的网卡，实际上是操作系统用软件模拟的，它可以通过TCP/IP与同一台主机上的其他服务进行通信，以127开头的IPv4地址就是为它保留的，主流Linux操作系统为环回网卡分配的地址都是127.0.0.1，主机名是localhost。
环回网络接口之所以被称之为环回网络接口，是因为从本机发送到本机任意一个IP的数据报文都会在网络层交给环回网络接口，不再下发到数据链路层进行处理，环回网络接口直接发送回网络层，最终交由应用层软件程序进行处理。这种方式对于性能测试非常有用，因为省去了硬件的开销，可以直接测试协议栈软件所需要的时间。
那环回网络接口是如何判断目的IP是否为本机地址的呢？
答案就是网络层在进行路由转发的时候会先查本地的路由表，发现是本机IP后交给环回网络接口。查看本地路由表的命令如下：
ip route show table local 输出内容如下：
broadcast 10.141.128.0 dev eth0 proto kernel scope link src 10.141.155.131 local 10.141.155.131 dev eth0 proto kernel scope host src 10.141.155.131 broadcast 10.141.191.255 dev eth0 proto kernel scope link src 10.141.155.131 broadcast 127.0.0.0 dev lo proto kernel scope link src 127.0.0.1 local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 local 127.0.0.1 dev lo proto kernel scope host src 127.</description>
    </item>
    
    <item>
      <title>Linux 修改最大文件描述符</title>
      <link>https://typesafe.cn/posts/linux-limit/</link>
      <pubDate>Mon, 11 Jan 2021 15:40:41 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/linux-limit/</guid>
      <description>echo &amp;#34;fs.file-max=655350&amp;#34; &amp;gt;&amp;gt;/etc/sysctl.conf echo &amp;#34;* soft nofile 655350&amp;#34; &amp;gt;&amp;gt; /etc/security/limits.conf echo &amp;#34;* hard nofile 655350&amp;#34; &amp;gt;&amp;gt; /etc/security/limits.conf ulimit -n 655350 </description>
    </item>
    
    <item>
      <title>压缩qcow2 镜像文件</title>
      <link>https://typesafe.cn/posts/qcow2-image-compression/</link>
      <pubDate>Thu, 31 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/qcow2-image-compression/</guid>
      <description>压缩qcow2    首先，需要对虚拟机剩余空间进行写零操作：
dd if=/dev/zero of=/zero.dat 删除 zero.dat：
rm /zero.dat 关闭虚拟机，执行qemu-img的convert命令进行转换：
qemu-img convert -c -O qcow2 /path/old.img.qcow2 /path/new.img.qcow2 </description>
    </item>
    
    <item>
      <title>Open vSwitch 入门实践（6）VXLAN实验</title>
      <link>https://typesafe.cn/posts/ovs-learn-6/</link>
      <pubDate>Wed, 30 Dec 2020 19:14:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/ovs-learn-6/</guid>
      <description>什么是VXLAN？    VXLAN是一种隧道封装协议，在三层网络上封装二层网络数据报文。简单来说就是可以在已经规划好网络拓扑的设备上封装出一个新的二层网络，因此VXLAN这类网络又被称之为overylay网络，底下承载VXLAN网络的就被称之为underlay网络。
VXLAN解决了什么问题？    最近几年，阿里云，腾讯云，京东云，华为云等等厂商每到节日都会打折出售大量云服务器，1核1G内存50G磁盘的服务器几十块就能买到一年的使用权，作为一个专业的羊毛党，哪个手里没有几台小破水管机器？但是这么多的云服务器是厂商如何做隔离的呢？了解过网络的同学或许会说VLAN。但是VLAN这种只能隔离4094个虚拟网络的技术别说满足不了羊毛党了，就连正常的用户估计都撑不住。那不隔离能行吗，厂商规划一个特别大的网段，让大家都在这里面耍，正常用户还好，万一这个时候进来一个大黑客，估计就会全部GG。
因此，隔离是必不可少的，其中关键的技术就是overlay网络。
那VXLAN具体解决了哪些问题呢？
 突破了VLAN技术4094个隔离网络的限制，在一个管理域中创建多达1600万个VXLAN网络。 VXLAN提供了云服务厂商所需的规模的网络分段，以支持大量租户。 突破了物理网络边界的限制，传统虚拟二层网络（VLAN）是需要和物理网络做大量适配工作才能保证环境的迁移不会导致虚拟网络异常，overlay网络则不必关心底层物理网络是如何搭建的，只要能保证VXLAN端点相互之间可以联通即可。  VXLAN网络如何工作？    VXLAN隧道协议将二层以太网帧封装在三层UDP数据包中，使用户能够创建跨物理三层网络的虚拟化二层子网或网段。每个二层子网使用VXLAN网络标识符（VNI）作为唯一标识。报文格式如下图：
执行数据包封装和解封装的实体称为VXLAN隧道终结点（VTEP）。VTEP主要分为两类：硬件VTEP和软件VTEP。硬件VTEP我接触较少，这里就不再介绍了。
软件VTEP如下图所示：VTEP在数据包到达虚拟机之前进行了封装和解封装，使得虚拟机完全不需要知道VXLAN隧道以及它们之间的三层网络。
简单VXLAN实验    我们参照下图完成实验。
主机A    # 创建隧道网桥 ovs-vsctl add-br br-tun # 创建隧道端口并指定远端IP和VXLAN ID ovs-vsctl add-port br-tun vx01 -- set Interface vx01 type=vxlan options:remote_ip=192.168.123.232 options:key=1111 # 创建内部端口 ovs-vsctl add-port br-tun vnet0 -- set Interface vnet0 type=internal # 创建netns用于模拟虚拟网络设备 ip netns add ns0 # 将内部端口移动到netns中 ip link set vnet0 netns ns0 # 启动网卡 ip netns exec ns0 ip link set lo up ip netns exec ns0 ip link set vnet0 up # 配置IP ip netns exec ns0 ip addr add 192.</description>
    </item>
    
    <item>
      <title>Open vSwitch 入门实践（5）OVS Flow Table 流表规则</title>
      <link>https://typesafe.cn/posts/ovs-learn-5/</link>
      <pubDate>Tue, 29 Dec 2020 18:38:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/ovs-learn-5/</guid>
      <description>OpenvSwitch flow table 流表     OpenFlow（OF）被认为是第一个软件定义网络（SDN）标准之一。它最初在SDN环境中定义了通信协议，使SDN控制器能够与物理和虚拟的交换机和路由器等网络设备的转发平面直接进行交互，从而更好地适应不断变化的业务需求。
 如果把OpenFlow控制器比作“大脑”，OVS流表就像是“大腿”一样接受来自“大脑”的指令，决定要向哪个方向前进。但OVS流表功能更加强大，在没有OpenFlow控制器时，也可以自主工作，它本身也供一些命令让我们可以直接管理流表。
操作命令    查看流表规则    # 查看br-tun上的全部流表规则 ovs-ofctl dump-flows br-tun 添加或修改流表规则    ovs-ofctl add−flow／add−flows／mod−flows “流表匹配条件,actions=[动作1][,动作2…]” 如果你有过编程的经验，流表规则其实就是一个个简单的if语句，伪代码如下。
if (流表匹配条件){ 动作1， 动作2... } if (流表匹配条件){ 动作1， 动作2... } 删除流表规则    # 删除br-tun上的全部流表规则 ovs-ofctl del-flows br-tun # 删除br-tun上匹配xx的全部流表规则 ovs-ofctl del-flows br-tun xx 流表匹配条件    OVS 流表匹配条件较多，下面我将其分成四部分来说明，分别是:
 OVS匹配条件 OSI模型第二层【数据链路层】 OSI模型第三层【网络层】 OSI模型第四层【传输层】  OVS匹配条件    in_port=port    流量进入的端口编号或者名称，示例 in_port=br-int</description>
    </item>
    
    <item>
      <title>Open vSwitch 入门实践（4）使用OVS配置端口镜像</title>
      <link>https://typesafe.cn/posts/ovs-learn-4/</link>
      <pubDate>Mon, 28 Dec 2020 19:23:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/ovs-learn-4/</guid>
      <description>前言    当我们想要在不影响虚拟网络设备数据报文收发的情况下获取对应虚拟网络设备的流量时，端口镜像是一个很好的选择。端口镜像是指将经过指定端口（镜像端口）的报文复制一份到另一个指定端口（观察端口），通过观察端口接收到的数据报文，就可以有效识别虚拟网络的运行情况。
OVS提供了相关命令来配置或删除端口镜像，下面我们来实验一下。
如何使用    端口镜像类型    端口镜像分为镜像源和镜像目的两部分。
镜像源     select_all：布尔类型（true，false）。设置为 true 时，表示此网桥上的所有流量。 select_dst_port：字符串（端口名称）。表示此端口接收的所有流量。 select_src_port：字符串（端口名称）。表示此端口发送的所有流量。 select_vlan：整型（0-4095）。表示携带此VLAN标签的流量。  镜像目的     output_port：字符串（端口名称）。接收流量报文的观察端口。 output_vlan：整型（0-4095）。表示只修改VLAN标签，原VLAN标签会被剥离。  基础操作命令    新增端口镜像
ovs-vsctl -- set Bridge &amp;lt;bridge_name&amp;gt; mirrors=@m \  -- --id=@&amp;lt;port0&amp;gt; get Port &amp;lt;port0&amp;gt; \  -- --id=@&amp;lt;port1&amp;gt; get Port &amp;lt;port1&amp;gt; \  -- --id=@m create Mirror name=&amp;lt;mirror_name&amp;gt; select-dst-port=@&amp;lt;port0&amp;gt; select-src-port=@&amp;lt;port0&amp;gt; output-port=@&amp;lt;port1&amp;gt;  这行命令会输出一个镜像ID</description>
    </item>
    
    <item>
      <title>Open vSwitch 入门实践（3）使用OVS构建分布式隔离网络</title>
      <link>https://typesafe.cn/posts/ovs-learn-3/</link>
      <pubDate>Sun, 27 Dec 2020 11:38:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/ovs-learn-3/</guid>
      <description>使用OVS构建分布式隔离网络    前言    上一节我们使用OVS构建了单机隔离网络，但是随着网络规模的扩张，单节点已经不再能满足业务的需要，分布式网络成了必不可少的环节。分布式网络与单节点网络在细节实现上基本一致，只有物理环境网络连线上的一点区别。
实验1：分布式无隔离网络    网络拓扑如下图所示，我们每一台节点都有两张网卡，一张用于管理，一张用于业务。之所以使用两张网卡有两个原因：
 管理网卡用于日常的维护登录，业务网卡用于传输虚拟节点的数据报文，避免相互之间影响。 我们要将业务网卡绑定到OVS网桥上，也就是Normal类型的Port。这种方式添加的Port不支持分配IP地址，如果之前网卡上配置的有IP，挂载到OVS上面之后将不可访问。   需要注意的是，如果是使用物理环境搭建网络拓扑，需要把业务网卡对应的交换机端口配置为trunk模式。如果是使用VmWare搭建网络拓扑，业务网卡需要配置网络类型为仅主机模式。
 配置     配置环境 主机A  ovs-vsctl add-br br-int # 请修改eth1为当前实验环境的业务网卡名称 ovs-vsctl add-port br-int eth1 # 添加两个内部端口 ovs-vsctl add-port br-int vnet0 -- set Interface vnet0 type=internal ovs-vsctl add-port br-int vnet1 -- set Interface vnet1 type=internal # 添加两个netns ip netns add ns0 ip netns add ns1 # 将内部端口分别移动到netns中 ip link set vnet0 netns ns0 ip link set vnet1 netns ns1 # 启动端口并配置IP ip netns exec ns0 ip link set lo up ip netns exec ns0 ip link set vnet0 up ip netns exec ns0 ip addr add 10.</description>
    </item>
    
    <item>
      <title>Open vSwitch 入门实践（2）使用OVS构建隔离网络</title>
      <link>https://typesafe.cn/posts/ovs-learn-2/</link>
      <pubDate>Thu, 26 Nov 2020 22:55:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/ovs-learn-2/</guid>
      <description>前言    在前面我们已经使用Linux Bridge完成了多台网络设备的通信，但是它对于网络隔离的支持不是很好，长期以来，在Linux平台上缺少一个功能完备的虚拟交换机，直到OVS的出现。
实验    接下来我们来尝试完成两个实验，单机无隔离网络、单机隔离网络。
实验一：单机无隔离网络    使用ovs构建无隔离网络非常简单，只需要添加一个网桥，然后在这个网桥上再增加几个内部端口，最后把端口移动到netns中即可。
# 添加网桥 ovs-vsctl add-br br-int # 添加三个内部端口 ovs-vsctl add-port br-int vnet0 -- set Interface vnet0 type=internal ovs-vsctl add-port br-int vnet1 -- set Interface vnet1 type=internal ovs-vsctl add-port br-int vnet2 -- set Interface vnet2 type=internal # 添加三个netns ip netns add ns0 ip netns add ns1 ip netns add ns2 # 将内部端口分别移动到netns中 ip link set vnet0 netns ns0 ip link set vnet1 netns ns1 ip link set vnet2 netns ns2 # 启动端口并配置IP ip netns exec ns0 ip link set lo up ip netns exec ns0 ip link set vnet0 up ip netns exec ns0 ip addr add 10.</description>
    </item>
    
    <item>
      <title>Open vSwitch 入门实践（1）Open vSwitch 是什么</title>
      <link>https://typesafe.cn/posts/ovs-learn-1/</link>
      <pubDate>Wed, 25 Nov 2020 17:49:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/ovs-learn-1/</guid>
      <description>OVS简介    Open vSwitch 是什么？    Open vSwitch(以下简称OVS)是一个用C语言开发的多层虚拟交换机，使用Apcahe 2开源许可证，现如今基本上已经成为了开源SDN（软件定义网络）基础设施层的事实标准。
OVS支持哪些功能？     支持NetFlow、sFlow(R)、IPFIX、SPAN、RSPAN和GRE隧道镜像等多种流量监控协议 支持LACP (IEEE 802.1AX-2008) 支持标准802.1Q VLAN协议，允许端口配置trunk模式 支持组播 支持BFD和802.1ag链路监控 支持STP（IEEE 802.1D-1998）和RSTP（IEEE 802.1D-2004） 支持细粒度的QoS（服务质量）配置 支持HFSC qdisc 支持接管每一个虚拟机的流量 支持基于源MAC的负载均衡、主备模式和L4哈希的端口绑带 支持OpenFlow协议（包含了很多对虚拟化的扩展） 支持IPv6 支持多种隧道协议（GRE、VXLAN、STT、Geneve和IPsec） 支持C和Python的远程配置协议 支持内核和用户空间的转发引擎选项 具有流缓存引擎的多表转发管道 转发层抽象以简化向新软件和硬件平台的移植  OVS的术语解释    Bridge    中文名称网桥，一个Bridge代表一个以太网交换机（Switch），一台主机中可以创建一个或多个Bridge，Bridge可以根据一定的规则，把某一个端口接收到的数据报文转发到另一个或多个端口上，也可以修改或者丢弃数据报文。
Port    中文名称端口，需要注意的是它和TCP里面的端口不是同样的概念，它更像是物理交换机上面的插口，可以接水晶头的那种。Port隶属于Bridge，必须先添加了Bridge才能在Bridge上添加Port。Port有以下几种类型：
  Normal
用户可以把操作系统中已有的网卡添加到Open vSwicth上，Open vSwitct会自动生成一个同名的Port开处理这张网卡进和出的数据报文。
 不过需要注意的是这种方式添加的Port不支持分配IP地址，如果之前网卡上配置的有IP，挂载到OVS上面之后将不可访问。此类型的Port常用于VLAN模式的多台物理主机相连的那个口，交换机一端属于Trunk模式。
   Internal
当Port的类型是Internal时，OVS会自动创建一个虚拟网卡（Interface），此端口收到的数据报文都会转发给这块网卡，从这块网卡发出的数据报文也会通过Port交给OVS处理。当OVS创建一个新的网桥时，会自动创建一个与网桥同名的Internal Port，同时也会创建一个与网桥同名的Interface，因此可以通过ip命令在操作系统中查看到这张虚拟网卡，但是状态是down的。</description>
    </item>
    
    <item>
      <title>服务器不允许上网并且需要跳板机才能访问？学会使用这个工具，轻松让服务器使用yum。</title>
      <link>https://typesafe.cn/posts/4dnat/</link>
      <pubDate>Thu, 19 Nov 2020 22:33:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/4dnat/</guid>
      <description>前言    你是否遇到过这样的场景，服务器不能上网，但是又需要安装某个软件，面对如蛛网般杂乱的rpm包依赖关系，放弃或许是最好的选择，这样你就不必再为无法完成工作而痛苦又懊恼。
但是今天，你有了一个更好的选择。
4DNAT    4DNAT取名源自4和DNAT。这个工具工作在OSI模型的第四层传输层，同时4和for谐音，意为专门为目标地址转换而服务的工具。4DNAT使用go语言开发，具有天然的跨平台性，并且完全使用go标准库开发，没有任何的第三方依赖，编译之后只有一个二进制可执行文件。它有4种工作模式：
转发模式    接受两个参数，监听端口和目标地址，在监听端口接收到请求后会主动连接目标地址，示例：
./4dnat -forward 2222 192.168.1.100:22 监听模式    接受两个参数，监听端口1和监听端口2，并交换两个端口接收到的数据，示例：
./4dnat -listen 10000 10001 代理人模式    接受两个参数，目标地址1和目标地址2，启动后会主动连接这两个目标地址，并交换两个端口接收到的数据，示例：
./4dnat -agent 127.0.0.1:10000 127.0.0.1:22 http/https代理模式    接受两个参数或四个参数，代理类型、监听端口、证书路径和私钥路径，示例：
http代理    ./4dnat -proxy http 1080 https代理    ./4dnat -proxy https 1080 server.crt server.key 使用场景    场景一    期望可以在用户电脑上直接访问目标服务器上的3306端口，跳板机器是一台Windows机器，没办法做ssh端口转发。  单向虚线箭头表示可以单向访问，反之不行。</description>
    </item>
    
    <item>
      <title>Docker？Vmware？小孩子才做选择，打工人我全都要。</title>
      <link>https://typesafe.cn/posts/vctl/</link>
      <pubDate>Tue, 17 Nov 2020 15:39:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/vctl/</guid>
      <description>前言    作为一个称职的打工人，电脑上常备一个Vmware不是什么新鲜事了，但是它和Docker for Windows不兼容往往很让人头大。通过查找资料，发现提供的解决方案大致有三种
 先使用Vmware创建一台Linux虚拟机，在这台Linux虚拟机上再安装docker。 配置Vmware作为Docker for Windows的运行平台。 使用微软的Hyper-v来创建虚拟机。  对我而言，第一种不太优雅，第二种配置繁琐，第三种不会用。
直到我发现了vctl这个好东西。
vctl 是什么？     vctl 是一款捆绑在Vmware Workstation Pro 应用程序中的命令行实用程序，仅在 Windows 10 1809 或更高版本上受支持。如果 Workstation Pro 所在主机上的 Windows 操作系统低于 Windows 10 1809，则它不支持 vctl CLI。
 简单来说它就是Vmware上的一个工具，可以用它来管理容器，使用命令基本上和docker一致，只需要把docker &amp;lt;cmd&amp;gt;换成vctl &amp;lt;cmd&amp;gt;就足够了。Docker for Windows？不需要。现在容器都交给vctl来管理了。
在使用vctl命令前，和启动docker一样，需要先启动vctl的守护进程。
vctl system start 当需要关闭守护进程时执行
vctl system stop 接下来就是和普通的docker命令一样了。
# 拉取镜像 vctl pull nginx # 查看镜像 vctl images # 启动容器 vctl --name some-nginx -d -p 8080:80 nginx # 查看容器 vctl ps # 进入容器 vctl exec -it &amp;lt;cid&amp;gt; bash 更多使用信息可参考Vmware的官方文档 使用vctl命令管理容器</description>
    </item>
    
    <item>
      <title>Linux Bridge 详解</title>
      <link>https://typesafe.cn/posts/linux-bridge/</link>
      <pubDate>Fri, 13 Nov 2020 21:47:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/linux-bridge/</guid>
      <description>Linux Bridge 详解    Linux Bridge（网桥）是用纯软件实现的虚拟交换机，有着和物理交换机相同的功能，例如二层交换，MAC地址学习等。因此我们可以把tun/tap，veth pair等设备绑定到网桥上，就像是把设备连接到物理交换机上一样。此外它和veth pair、tun/tap一样，也是一种虚拟网络设备，具有虚拟设备的所有特性，例如配置IP，MAC地址等。
Linux Bridge通常是搭配KVM、docker等虚拟化技术一起使用的，用于构建虚拟网络，因为此教程不涉及虚拟化技术，我们就使用前面学习过的netns来模拟虚拟设备。
如何使用Linux Bridge？    操作网桥有多种方式，在这里我们介绍一下通过bridge-utils来操作，由于它不是Linux系统自带的工具，因此需要我们手动来安装它。
# centos yum install -y bridge-utils # ubuntu apt-get install -y bridge-utils 使用brctl help查看使用帮助
never heard of command [help] Usage: brctl [commands] commands: addbr &amp;lt;bridge&amp;gt;	add bridge delbr &amp;lt;bridge&amp;gt;	delete bridge addif &amp;lt;bridge&amp;gt; &amp;lt;device&amp;gt;	add interface to bridge delif &amp;lt;bridge&amp;gt; &amp;lt;device&amp;gt;	delete interface from bridge hairpin &amp;lt;bridge&amp;gt; &amp;lt;port&amp;gt; {on|off}	turn hairpin on/off setageing &amp;lt;bridge&amp;gt; &amp;lt;time&amp;gt;	set ageing time setbridgeprio	&amp;lt;bridge&amp;gt; &amp;lt;prio&amp;gt;	set bridge priority setfd &amp;lt;bridge&amp;gt; &amp;lt;time&amp;gt;	set bridge forward delay sethello &amp;lt;bridge&amp;gt; &amp;lt;time&amp;gt;	set hello time setmaxage &amp;lt;bridge&amp;gt; &amp;lt;time&amp;gt;	set max message age setpathcost	&amp;lt;bridge&amp;gt; &amp;lt;port&amp;gt; &amp;lt;cost&amp;gt;	set path cost setportprio	&amp;lt;bridge&amp;gt; &amp;lt;port&amp;gt; &amp;lt;prio&amp;gt;	set port priority show [ &amp;lt;bridge&amp;gt; ]	show a list of bridges showmacs &amp;lt;bridge&amp;gt;	show a list of mac addrs showstp &amp;lt;bridge&amp;gt;	show bridge stp info stp &amp;lt;bridge&amp;gt; {on|off}	turn stp on/off 常用命令如</description>
    </item>
    
    <item>
      <title>Linux veth pair 详解</title>
      <link>https://typesafe.cn/posts/linux-veth-pair/</link>
      <pubDate>Mon, 09 Nov 2020 22:45:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/linux-veth-pair/</guid>
      <description>Linux veth pair 详解    veth pair是成对出现的一种虚拟网络设备接口，一端连着网络协议栈，一端彼此相连。如下图所示：
由于它的这个特性，常常被用于构建虚拟网络拓扑。例如连接两个不同的网络命名空间(netns)，连接docker容器，连接网桥(Bridge)等，其中一个很常见的案例就是OpenStack Neutron底层用它来构建非常复杂的网络拓扑。
如何使用？    创建一对veth
ip link add &amp;lt;veth name&amp;gt; type veth peer name &amp;lt;peer name&amp;gt; 实验    我们改造上一节完成的netns实验，使用veth pair将两个的隔离netns连接起来。如下图所示：
我们首先创建一对veth设备，将veth设备分别移动到两个netns中并启动。
# 创建一对veth ip link add veth0 type veth peer name veth1 # 将veth移动到netns中 ip link set veth0 netns ns0 ip link set veth1 netns ns1 # 启动 ip netns exec ns0 ip link set veth0 up ip netns exec ns1 ip link set veth1 up 接下来我们测试一下。</description>
    </item>
    
    <item>
      <title>Linux Network Namespace (netns) 详解</title>
      <link>https://typesafe.cn/posts/linux-netns/</link>
      <pubDate>Sun, 08 Nov 2020 23:12:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/linux-netns/</guid>
      <description>Network Namespace （以下简称netns）是Linux内核提供的一项实现网络隔离的功能，它能隔离多个不同的网络空间，并且各自拥有独立的网络协议栈，这其中便包括了网络接口（网卡），路由表，iptables规则等。例如大名鼎鼎的docker便是基于netns实现的网络隔离，今天我们就来手动实验一下netns的隔离特性。
使用方式    使用ip netns help查看使用帮助
Usage: ip netns list ip netns add NAME ip netns set NAME NETNSID ip [-all] netns delete [NAME] ip netns identify [PID] ip netns pids NAME ip [-all] netns exec [NAME] cmd ... ip netns monitor ip netns list-id 开始实验    我们将要构建如下图的网络
首先我们添加两个tap设备并配置上IP信息，然后添加两个netns，最后将tap设备移动到netns中
# 添加并启动虚拟网卡tap设备 ip tuntap add dev tap0 mode tap ip tuntap add dev tap1 mode tap ip link set tap0 up ip link set tap1 up # 配置IP ip addr add 10.</description>
    </item>
    
    <item>
      <title>Linux tun/tap 详解</title>
      <link>https://typesafe.cn/posts/linux-tun-tap/</link>
      <pubDate>Sun, 08 Nov 2020 22:11:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/linux-tun-tap/</guid>
      <description>在计算机网络中，tun与tap是操作系统内核中的虚拟网络设备。不同于普通靠硬件网络适配器实现的设备，这些虚拟的网络设备全部用软件实现，并向运行于操作系统上的软件提供与硬件的网络设备完全相同的功能。
 tun/tap是什么？    tun是网络层的虚拟网络设备，可以收发第三层数据报文包，如IP封包，因此常用于一些点对点IP隧道，例如OpenVPN，IPSec等。
tap是链路层的虚拟网络设备，等同于一个以太网设备，它可以收发第二层数据报文包，如以太网数据帧。Tap最常见的用途就是做为虚拟机的网卡，因为它和普通的物理网卡更加相近，也经常用作普通机器的虚拟网卡。
如何操作tun/tap？    Linux tun/tap可以通过网络接口和字符设备两种方式进行操作。
当应用程序使用标准网络接口socket API操作tun/tap设备时，和操作一个真实网卡无异。
当应用程序使用字符设备操作tun/tap设备时，字符设备即充当了用户空间和内核空间的桥梁直接读写二层或三层的数据报文。在 Linux 内核 2.6.x 之后的版本中，tun/tap 对应的字符设备文件分别为：
tun：/dev/net/tun tap：/dev/tap0 当应用程序打开字符设备时，系统会自动创建对应的虚拟设备接口，一般以tunX和tapX方式命名，虚拟设备接口创建成功后，可以为其配置IP、MAC地址、路由等。当一切配置完毕，应用程序通过此字符文件设备写入IP封包或以太网数据帧，tun/tap的驱动程序会将数据报文直接发送到内核空间，内核空间收到数据后再交给系统的网络协议栈进行处理，最后网络协议栈选择合适的物理网卡将其发出，到此发送流程完成。而物理网卡收到数据报文时会交给网络协议栈进行处理，网络协议栈匹配判断之后通过tun/tap的驱动程序将数据报文原封不动的写入到字符设备上，应用程序从字符设备上读取到IP封包或以太网数据帧，最后进行相应的处理，收取流程完成。
 注意：当应用程序关闭字符设备时，系统也会自动删除对应的虚拟设备接口，并且会删除掉创建的路由等信息。
 tun/tap的区别    tun/tap 虽然工作原理一致，但是工作的层次不一样。
tun是三层网络设备，收发的是IP层数据包，无法处理以太网数据帧，例如OpenVPN的路由模式就是使用了tun网络设备，OpenVPN Server重新规划了一个网段，所有的客户端都会获取到该网段下的一个IP，并且会添加对应的路由规则，而客户端与目标机器产生的数据报文都要经过OpenVPN网关才能转发。
tap是二层网络设备，收发以太网数据帧，拥有MAC层的功能，可以和物理网卡通过网桥相连，组成一个二层网络。例如OpenVPN的桥接模式可以从外部打一条隧道到本地网络。进来的机器就像本地的机器一样参与通讯，丝毫看不出这些机器是在远程。如果你有使用过虚拟机的经验，桥接模式也是一种十分常见的网络方案，虚拟机会分配到和宿主机器同网段的IP，其他同网段的机器也可以通过网络访问到这台虚拟机。
使用方式    Linux 提供了一些命令行程序方便我们来创建持久化的tun/tap设备，但是如果没有应用程序打开对应的文件描述符，tun/tap的状态一直会是DOWN，还好的是这并不会影响我们把它当作普通网卡去使用。
使用ip tuntap help查看使用帮助
Usage: ip tuntap { add | del | show | list | lst | help } [ dev PHYS_DEV ] [ mode { tun | tap } ] [ user USER ] [ group GROUP ] [ one_queue ] [ pi ] [ vnet_hdr ] [ multi_queue ] [ name NAME ] Where:	USER := { STRING | NUMBER } GROUP := { STRING | NUMBER } 示例    # 创建 tap  ip tuntap add dev tap0 mode tap # 创建 tun ip tuntap add dev tun0 mode tun # 删除 tap ip tuntap del dev tap0 mode tap # 删除 tun ip tuntap del dev tun0 mode tun tun/tap 设备创建成功后可以当作普通的网卡一样使用，因此我们也可以通过ip link命令来操作它。</description>
    </item>
    
    <item>
      <title>tcpkill在go语言下的实现和增强</title>
      <link>https://typesafe.cn/posts/tcpwall/</link>
      <pubDate>Sat, 31 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://typesafe.cn/posts/tcpwall/</guid>
      <description>tcpwall    当我们想要阻止某些TCP连接的建立，在Linux平台上有一个很好的解决方案iptables，但是对那些已经建立的tcp连接，iptables就不能做到随心所欲的阻断了。
我在互联网上检索的时候发现了tcpkill这个工具，tcpkill是一个网络分析工具集dsniff中的一个小工具。在Linux上可以直接通过dsniff包安装，使用方式也非常简单。
通过测试我发现tcpkill在执行命令之后并不会立刻阻断tcp连接，而是等待有数据传输时，才会阻断，因此在执行完命令之后程序并不会主动退出，而是需要通过Ctrl+C来退出，这对于某些想要通过程序来调用的脚本小子（例如我）来说简直是个灾难。
如何阻断一个已经建立的tcp连接？    阻断一个已经建立的tcp连接通常有这几种方案：
 服务端主动断开 客户端主动断开 拔掉网线（时间要超过tcp超时时间） 伪造RST数据包发送给服务端和客户端让它们主动断开（tcpkill就是这么做的）  前三种局限性太大，只能用第4种了。
如何实现伪造RST数据报文包？    GoPacket 是go基于libpcap构建的一个库，可以通过旁路的方式接收一份数据包的拷贝。因此我们可以很方便捕获到正在通信的tcp数据报文。通过数据报文，我们可以获取到通信双方的MAC地址，IP和端口号，以及ACK号等，这些都是伪造数据包必不可少的。
在学习了tcpkill的源码之后，我使用go开发了一个增强版的tcpwall，tcpwall不仅可以实现和tcpkill同样的基于ip或端口监听到指定数据报文之后伪造RST数据报文来阻断tcp连接，也可以通过源ip源端口，目的ip目的端口来主动发送SYN数据报文包来诱导那些没有数据的tcp连接发送ACK数据报文包以获取源MAC、目的MAC和ACK号，并且可以通过指定参数让程序等待一段时间后主动退出。
如何使用    阻断指定IP和端口的TCP连接（不关心是源或者目的）
tcpwall -i {interface} -host {host} -port {port} 阻断指定源IP和源端口的TCP连接
tcpwall -i {interface} -shost {src_host} -sport {src_port} 阻断指定目的IP和目的端口的TCP连接
tcpwall -i {interface} -dhost {dst_host} -dport {dst_port} 阻断指定源IP、源端口、目的IP、目的端口的TCP连接（会主动向双方发送SYN数据报文包）
tcpwall -i {interface} -shost {src_host} -sport {src_port} -dhost {dst_host} -dport {dst_port} 其他     -timeout 时间（秒）指定等待多久之后退出程序  项目地址 https://github.</description>
    </item>
    
  </channel>
</rss>
